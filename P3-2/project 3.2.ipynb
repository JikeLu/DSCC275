{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdb776f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arff2pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, optim\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marff2pandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m a2p\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# %matplotlib inline\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# %config InlineBackend.figure_format='retina'\u001b[39;00m\n\u001b[0;32m     40\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m'\u001b[39m, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmuted\u001b[39m\u001b[38;5;124m'\u001b[39m, font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'arff2pandas'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Time Series Anomaly Detection using LSTM Autoencoders with PyTorch in Python\n",
    "\n",
    "# Uncomment the \"pip\" commands as necessary to install the packages\n",
    "\n",
    "# Needed to access the data files\n",
    "# !pip install -qq arff2pandas\n",
    "\n",
    "# \n",
    "# !pip install -q -U watermark\n",
    "\n",
    "# !pip install -qq -U pandas\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %reload_ext watermark\n",
    "# %watermark -v -p numpy,pandas,torch,arff2pandas\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from arff2pandas import a2p\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "\"\"\"\n",
    "In this tutorial, you'll learn how to detect anomalies in Time Series data using an LSTM Autoencoder. \n",
    "You're going to use real-world ECG data from a single patient with heart disease to detect abnormal hearbeats.\n",
    "\n",
    "# Data\n",
    "The [dataset](http://timeseriesclassification.com/description.php?Dataset=ECG5000) contains 5,000 Time Series examples (obtained with ECG) with 140 timesteps. Each sequence corresponds to a single heartbeat from a single patient with congestive heart failure.\n",
    "\n",
    "> An electrocardiogram (ECG or EKG) is a test that checks how your heart is functioning by measuring the electrical activity of the heart. With each heart beat, an electrical impulse (or wave) travels through your heart. This wave causes the muscle to squeeze and pump blood from the heart. [Source](https://www.heartandstroke.ca/heart/tests/electrocardiogram)\n",
    "\n",
    "We have 5 types of hearbeats (classes):\n",
    "\n",
    "- Normal (N) \n",
    "- R-on-T Premature Ventricular Contraction (R-on-T PVC)\n",
    "- Premature Ventricular Contraction (PVC)\n",
    "- Supra-ventricular Premature or Ectopic Beat (SP or EB) \n",
    "- Unclassified Beat (UB).\n",
    "\n",
    "> Assuming a healthy heart and a typical rate of 70 to 75 beats per minute, each cardiac cycle, or heartbeat, takes about 0.8 seconds to complete the cycle.\n",
    "Frequency: 60–100 per minute (Humans)\n",
    "Duration: 0.6–1 second (Humans) [Source](https://en.wikipedia.org/wiki/Cardiac_cycle)\n",
    "\"\"\"\n",
    "\n",
    "#Load the arff files into Pandas data frames\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with open(r'ECG5000_TRAIN.arff') as f:\n",
    "    train = a2p.load(f)\n",
    "\n",
    "with open(r'ECG5000_TEST.arff') as f:\n",
    "    test = a2p.load(f)\n",
    "\n",
    "#We'll combine the training and test data into a single data frame. This will give us more data to train our Autoencoder. We'll also shuffle it:\"\"\"\n",
    "\n",
    "df = train.append(test)\n",
    "df = df.sample(frac=1.0)\n",
    "df.shape\n",
    "\n",
    "df.head()\n",
    "\n",
    "\"\"\"We have 5,000 examples. Each row represents a single heartbeat record. Let's name the possible classes:\"\"\"\n",
    "\n",
    "CLASS_NORMAL = 1\n",
    "\n",
    "class_names = ['Normal','R on T','PVC','SP','UB']\n",
    "\n",
    "\"\"\"Next, we'll rename the last column to `target`, so its easier to reference it:\"\"\"\n",
    "\n",
    "new_columns = list(df.columns)\n",
    "new_columns[-1] = 'target'\n",
    "df.columns = new_columns\n",
    "\n",
    "\"\"\"## Exploratory Data Analysis\n",
    "\n",
    "Let's check how many examples for each heartbeat class do we have:\n",
    "\"\"\"\n",
    "\n",
    "df.target.value_counts()\n",
    "\n",
    "\"\"\"Let's plot the results:\"\"\"\n",
    "\n",
    "ax = sns.countplot(df.target)\n",
    "ax.set_xticklabels(class_names);\n",
    "\n",
    "\"\"\"The normal class, has by far, the most examples. \n",
    "\n",
    "Let's have a look at an averaged (smoothed out with one standard deviation on top and bottom of it) Time Series for each class:\n",
    "\"\"\"\n",
    "\n",
    "def plot_time_series_class(data, class_name, ax, n_steps=10):\n",
    "    time_series_df = pd.DataFrame(data)\n",
    "\n",
    "    smooth_path = time_series_df.rolling(n_steps).mean()\n",
    "    path_deviation = 2 * time_series_df.rolling(n_steps).std()\n",
    "\n",
    "    under_line = (smooth_path - path_deviation)[0]\n",
    "    over_line = (smooth_path + path_deviation)[0]\n",
    "\n",
    "    ax.plot(smooth_path, linewidth=2)\n",
    "    ax.fill_between(\n",
    "      path_deviation.index,\n",
    "      under_line,\n",
    "      over_line,\n",
    "      alpha=.125\n",
    "  )\n",
    "    ax.set_title(class_name)\n",
    "\n",
    "classes = df.target.unique()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=len(classes) // 3 + 1,\n",
    "  ncols=3,\n",
    "  sharey=True,\n",
    "  figsize=(14, 8)\n",
    ")\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    ax = axs.flat[i]\n",
    "    data = df[df.target == cls] \\\n",
    "    .drop(labels='target', axis=1) \\\n",
    "    .mean(axis=0) \\\n",
    "    .to_numpy()\n",
    "    plot_time_series_class(data, class_names[i], ax)\n",
    "\n",
    "fig.delaxes(axs.flat[-1])\n",
    "fig.tight_layout();\n",
    "\n",
    "## LSTM Autoencoder\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "#Let's get all normal heartbeats and drop the target (class) column:\n",
    "\n",
    "normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)\n",
    "normal_df.shape\n",
    "\n",
    "#Merge all other classes and mark them as anomalies:\"\"\"\n",
    "\n",
    "anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)\n",
    "anomaly_df.shape\n",
    "\n",
    "#Split the normal examples into train, validation and test sets:\"\"\"\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "  normal_df,\n",
    "  test_size=0.15,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "  val_df,\n",
    "  test_size=0.33, \n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "#Convert our examples into tensors, so we can use them to train our Autoencoder. \n",
    "\n",
    "def create_dataset(df):\n",
    "\n",
    "    sequences = df.astype(np.float32).to_numpy().tolist()\n",
    "\n",
    "    dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences]\n",
    "\n",
    "    n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
    "\n",
    "    return dataset, seq_len, n_features\n",
    "\n",
    "#Each Time Series will be converted to a 2D Tensor in the shape *sequence length* x *number of features* (140x1 in our case).\n",
    "\n",
    "#Create Train, Val and Test datasets:\n",
    "\n",
    "train_dataset, seq_len, n_features = create_dataset(train_df)\n",
    "val_dataset, _, _ = create_dataset(val_df)\n",
    "test_normal_dataset, _, _ = create_dataset(test_df)\n",
    "test_anomaly_dataset, _, _ = create_dataset(anomaly_df)\n",
    "\n",
    "# LSTM Autoencoder\n",
    "#The general Autoencoder architecture consists of two components. An *Encoder* that compresses the input and a *Decoder* that tries to reconstruct it.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "    #self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim, embedding_dim\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "\n",
    "    #x, (_, _) = self.rnn1(x)\n",
    "        x, (hidden_n, _) = self.rnn1(x)\n",
    "    #x, (hidden_n, _) = self.rnn2(x)\n",
    "\n",
    "        return hidden_n.reshape((self.n_features, self.embedding_dim))\n",
    "\n",
    "\"\"\"The *Encoder* uses LSTM layers to compress the Time Series data input.\n",
    "Next, we'll decode the compressed representation using a *Decoder*:\n",
    "\"\"\"\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    # self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "        self.hidden_dim, self.n_features = input_dim, n_features\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=input_dim, \n",
    "      num_layers=1, \n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    # self.rnn2 = nn.LSTM(\n",
    "    #   input_size=input_dim,\n",
    "    #   hidden_size=self.hidden_dim,\n",
    "    #   num_layers=1,\n",
    "    #   batch_first=True\n",
    "    # )\n",
    "\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(self.seq_len, self.n_features)\n",
    "        x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n",
    "\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    #x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "        x = x.reshape((self.seq_len, self.hidden_dim))\n",
    "\n",
    "        return self.output_layer(x)\n",
    "\n",
    "#Our Decoder contains LSTM layer and an output layer that gives the final reconstruction.\n",
    "\n",
    "#Time to wrap everything into an easy to use module:\n",
    "\n",
    "\n",
    "class RecurrentAutoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(RecurrentAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\"Our Autoencoder passes the input through the Encoder and Decoder. Let's create an instance of it:\"\"\"\n",
    "\n",
    "#model = RecurrentAutoencoder(seq_len, n_features, 128)\n",
    "#model = RecurrentAutoencoder(seq_len, n_features, 8)\n",
    "model = RecurrentAutoencoder(seq_len, n_features, 8)\n",
    "model = model.to(device)\n",
    "\n",
    "\"\"\"## Training\n",
    "\n",
    "Let's write a helper function for our training process:\n",
    "\"\"\"\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, n_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "  \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for seq_true in train_dataset:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for seq_true in val_dataset:\n",
    "\n",
    "                seq_true = seq_true.to(device)\n",
    "                seq_pred = model(seq_true)\n",
    "\n",
    "                loss = criterion(seq_pred, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history\n",
    "\n",
    "\"\"\"At each epoch, the training process feeds our model with all training examples and evaluates the performance on the validation set. Note that we're using a batch size of 1 (our model sees only 1 sequence at a time). We also record the training and validation set losses during the process.\n",
    "\n",
    "Note that we're minimizing the [L1Loss](https://pytorch.org/docs/stable/nn.html#l1loss), which measures the MAE (mean absolute error). Why? The reconstructions seem to be better than with MSE (mean squared error).\n",
    "\n",
    "We'll get the version of the model with the smallest validation error. Let's do some training:\n",
    "\"\"\"\n",
    "\n",
    "model, history = train_model(\n",
    "  model, \n",
    "  train_dataset, \n",
    "  val_dataset, \n",
    "  n_epochs=50\n",
    ")\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "\n",
    "ax.plot(history['train'])\n",
    "ax.plot(history['val'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.title('Loss over training epochs')\n",
    "plt.show();\n",
    "\n",
    "## Saving the model\n",
    "\n",
    "#Let's store the model for later use:\n",
    "\n",
    "\n",
    "MODEL_PATH = 'model.pth'\n",
    "\n",
    "torch.save(model, MODEL_PATH)\n",
    "\n",
    "\"\"\"## Choosing a threshold\n",
    "\n",
    "With our model at hand, we can have a look at the reconstruction error on the training set. Let's start by writing a helper function to get predictions from our model:\n",
    "\"\"\"\n",
    "\n",
    "def predict(model, dataset):\n",
    "    predictions, losses = [], []\n",
    "    criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for seq_true in dataset:\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_pred = model(seq_true)\n",
    "\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "            predictions.append(seq_pred.cpu().numpy().flatten())\n",
    "            losses.append(loss.item())\n",
    "    return predictions, losses\n",
    "\n",
    "\"\"\"Our function goes through each example in the dataset and records the predictions and losses. Let's get the losses and have a look at them:\"\"\"\n",
    "\n",
    "_, losses = predict(model, train_dataset)\n",
    "\n",
    "sns.distplot(losses, bins=50, kde=True);\n",
    "\n",
    "THRESHOLD = 45\n",
    "\n",
    "\"\"\"## Evaluation\n",
    "\n",
    "Using the threshold, we can turn the problem into a simple binary classification task:\n",
    "\n",
    "- If the reconstruction loss for an example is below the threshold, we'll classify it as a *normal* heartbeat\n",
    "- Alternatively, if the loss is higher than the threshold, we'll classify it as an anomaly\n",
    "\n",
    "### Normal hearbeats\n",
    "\n",
    "Let's check how well our model does on normal heartbeats. We'll use the normal heartbeats from the test set (our model haven't seen those):\n",
    "\"\"\"\n",
    "\n",
    "predictions, pred_losses = predict(model, test_normal_dataset)\n",
    "sns.distplot(pred_losses, bins=50, kde=True);\n",
    "\n",
    "\"\"\"We'll count the correct predictions:\"\"\"\n",
    "\n",
    "correct = sum(l <= THRESHOLD for l in pred_losses)\n",
    "print(f'Correct normal predictions: {correct}/{len(test_normal_dataset)}')\n",
    "\n",
    "\"\"\"### Anomalies\n",
    "\n",
    "We'll do the same with the anomaly examples, but their number is much higher. We'll get a subset that has the same size as the normal heartbeats:\n",
    "\"\"\"\n",
    "\n",
    "anomaly_dataset = test_anomaly_dataset[:len(test_normal_dataset)]\n",
    "#anomaly_dataset = test_anomaly_dataset\n",
    "\"\"\"Now we can take the predictions of our model for the subset of anomalies:\"\"\"\n",
    "\n",
    "predictions, pred_losses = predict(model, anomaly_dataset)\n",
    "sns.distplot(pred_losses, bins=50, kde=True);\n",
    "\n",
    "\"\"\"Finally, we can count the number of examples above the threshold (considered as anomalies):\"\"\"\n",
    "\n",
    "correct = sum(l > THRESHOLD for l in pred_losses)\n",
    "print(f'Correct anomaly predictions: {correct}/{len(anomaly_dataset)}')\n",
    "\n",
    "#### Looking at Examples\n",
    "\n",
    "#We can overlay the real and reconstructed Time Series values to see how close they are. We'll do it for some normal and anomaly cases:\n",
    "\n",
    "def plot_prediction(data, model, title, ax):\n",
    "    predictions, pred_losses = predict(model, [data])\n",
    "\n",
    "    ax.plot(data, label='true')\n",
    "    ax.plot(predictions[0], label='reconstructed')\n",
    "    ax.set_title(f'{title} (loss: {np.around(pred_losses[0], 2)})')\n",
    "    ax.legend()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "  nrows=2,\n",
    "  ncols=6,\n",
    "  sharey=True,\n",
    "  sharex=True,\n",
    "  figsize=(22, 8)\n",
    ")\n",
    "\n",
    "for i, data in enumerate(test_normal_dataset[:6]):\n",
    "    plot_prediction(data, model, title='Normal', ax=axs[0, i])\n",
    "\n",
    "for i, data in enumerate(test_anomaly_dataset[:6]):\n",
    "    plot_prediction(data, model, title='Anomaly', ax=axs[1, i])\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfb2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

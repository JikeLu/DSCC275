{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a52b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q2\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## DSC 275/475: Time Series Analysis and Forecasting (Fall 2020) \n",
    "# \n",
    "#  ## Project 3.2–Sequence Classification with Recurrent Neural Networks \n",
    "\n",
    "# Sample Code Block for Problem 1.1\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "names = {}\n",
    "languages = []\n",
    "\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# (TO DO:) CHANGE FILE PATH AS NECESSARY\n",
    "for filename in findFiles('names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    languages.append(category)\n",
    "    lines = readLines(filename)\n",
    "    names[category] = lines\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "n_categories = len(languages)\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def nameToTensor(name):\n",
    "    tensor = torch.zeros(len(name), 1, n_letters)\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS,OUTPUT_SIZE):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = HIDDEN_SIZE, # number of hidden units\n",
    "            num_layers = N_LAYERS, # number of layers\n",
    "            batch_first = True)\n",
    "        self.out = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        r_out, h = self.rnn(x, None) # None represents zero initial hidden state           \n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "allnames = [] # Create list of all names and corresponding output language\n",
    "for language in list(names.keys()):\n",
    "    for name in names[language]:\n",
    "        allnames.append([name, language])\n",
    "        \n",
    "## (TO DO:) Determine Padding length (this is the length of the longest string) 搞了\n",
    "\n",
    "# maxlen = ..... # Add code here to compute the maximum length of string    \n",
    "maxlen = 0\n",
    "for person in allnames:\n",
    "    length = len(person[0])\n",
    "    if length > maxlen:\n",
    "        maxlen = length\n",
    "                \n",
    "n_letters = len(all_letters)\n",
    "n_categories = len(languages)\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i.item()\n",
    "    return languages[category_i], category_i\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "rnn = RNN(n_letters, 128, 1, n_categories)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)   # optimize all rnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "\n",
    "for epoch in range(5):  \n",
    "    batch_size = len(allnames)\n",
    "    random.shuffle(allnames)\n",
    "    \n",
    "    # if \"b_in\" and \"b_out\" are the variable names for input and output tensors, you need to create those\n",
    "    \n",
    "    b_in_list = [batch_size, maxlen, n_letters]\n",
    "    b_out_list = [batch_size, n_categories]\n",
    "    b_in = torch.zeros(b_in_list) # 搞了 (TO DO:) Initialize \"b_in\" to a tensor with size of input (batch size, padded_length, n_letters)\n",
    "    b_out = torch.zeros(b_out_list, dtype=torch.long)# 搞了 (TO DO:) Initialize \"b_out\" to tensor with size (batch_size, n_categories, dtype=torch.long)       \n",
    "\n",
    "\n",
    "    # (TO DO:) Populate \"b_in\" tensor 这里\n",
    "\n",
    "    # (TO DO:) Populate \"b_out\" tensor 搞了\n",
    "    for i in range(batch_size):\n",
    "        b_out[i][languages.index(allnames[i][1])] = 1\n",
    "       \n",
    "\n",
    "    output = rnn(b_in)                               # rnn output\n",
    "    #(TO DO:)\n",
    "    loss = loss_func(output, ....)   # 这里 (TO DO:) Fill \"....\" to calculate the cross entropy loss\n",
    "    optimizer.zero_grad()                           # clear gradients for this training step\n",
    "    loss.backward()                                 # backpropagation, compute gradients\n",
    "    optimizer.step()                                # apply gradients\n",
    "        \n",
    "    # Print accuracy\n",
    "    test_output = rnn(b_in)                   # \n",
    "    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "    test_y = torch.max(b_out, 1)[1].data.numpy().squeeze()\n",
    "    accuracy = sum(pred_y == test_y)/batch_size\n",
    "    print(\"Epoch: \", epoch, \"| train loss: %.4f\" % loss.item(), '| accuracy: %.2f' % accuracy)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
